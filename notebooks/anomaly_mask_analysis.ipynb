{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Generation and Two-Branch Decoder Analysis\n",
    "\n",
    "Analyze the anomaly-generation and mask pipeline used in stage-2 training, and verify that:\n",
    "- **General decoder** outputs **x_g** (spectrogram **with** anomalies).\n",
    "- **Object-specific decoder** outputs **x_s** (spectrogram **without** anomalies).\n",
    "\n",
    "We use controlled fake anomalies to illustrate the training-phase process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and imports\n",
    "\n",
    "Paths, imports, and optional load of trained sDSR. If checkpoints are missing, the model is built with random weights (visualizations show \"signal flow\" only; the x_s \"without anomaly\" effect is strong only after training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda, n_mels=128, T=320, q_shape=(32, 80)\n"
     ]
    }
   ],
   "source": [
    "# Setup and paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "_cwd = Path(\".\").resolve()\n",
    "PROJECT_ROOT = _cwd.parent if _cwd.name == \"notebooks\" else _cwd\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CKPT_DIR = PROJECT_ROOT / \"checkpoints\"\n",
    "MACHINE_TYPE = \"fan\"\n",
    "STAGE1_CKPT = CKPT_DIR / \"stage1/ToyCar+ToyConveyor+fan+pump+slider+valve/stage1_ToyCar+ToyConveyor+fan+pump+slider+valve_final.pt\"\n",
    "STAGE2_CKPT = CKPT_DIR / \"stage2\" / MACHINE_TYPE / \"stage2_fan_best.pt\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_mels, T = 128, 320\n",
    "\n",
    "# q_shape used by sDSR (H = n_mels//4, W = T//4)\n",
    "def get_q_shape(n_mels, T):\n",
    "    return (max(1, n_mels // 4), max(1, T // 4))\n",
    "\n",
    "q_shape = get_q_shape(n_mels, T)\n",
    "H_q, W_q = q_shape[0], q_shape[1]\n",
    "spectrogram_shape = (n_mels, T)\n",
    "print(f\"Device: {DEVICE}, n_mels={n_mels}, T={T}, q_shape={q_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for sDSR:\n\tMissing key(s) in state_dict: \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.0._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.0._block.3.weight\", \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.1._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.1._block.3.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip2.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip2.bias\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip1.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip1.bias\", \"_object_decoder.spectrogram_reconstruction_network._conv_out.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_out.bias\". \n\tUnexpected key(s) in state_dict: \"_object_decoder.spectrogram_reconstruction_network.pre_vq_conv.weight\", \"_object_decoder.spectrogram_reconstruction_network.pre_vq_conv.bias\", \"_object_decoder.spectrogram_reconstruction_network.upblock1.weight\", \"_object_decoder.spectrogram_reconstruction_network.upblock1.bias\", \"_object_decoder.spectrogram_reconstruction_network.upblock2.weight\", \"_object_decoder.spectrogram_reconstruction_network.upblock2.bias\", \"_object_decoder.spectrogram_reconstruction_network._conv_1.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_1.bias\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.0._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.0._block.3.weight\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.1._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.1._block.3.weight\". \n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.0.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.0.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.3.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up2.1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up2.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.0.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up3.1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up3.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.fin_out.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.0.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.0.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.3.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up2.1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up2.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.0.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up3.1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up3.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.fin_out.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.0.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.3.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.3.weight: copying a param with shape torch.Size([1024, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans1.weight: copying a param with shape torch.Size([128, 64, 4, 4]) from checkpoint, the shape in current model is torch.Size([128, 128, 4, 4]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans2.weight: copying a param with shape torch.Size([64, 1, 4, 4]) from checkpoint, the shape in current model is torch.Size([128, 64, 4, 4]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans2.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m STAGE2_CKPT.exists():\n\u001b[32m     36\u001b[39m     ckpt_s2 = torch.load(STAGE2_CKPT, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_s2\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStage2 checkpoint not found; using random weights (signal-flow demo only).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/micromamba/envs/audio_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2576\u001b[39m         error_msgs.insert(\n\u001b[32m   2577\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2578\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2579\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2580\u001b[39m             ),\n\u001b[32m   2581\u001b[39m         )\n\u001b[32m   2583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2586\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2587\u001b[39m         )\n\u001b[32m   2588\u001b[39m     )\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for sDSR:\n\tMissing key(s) in state_dict: \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.0._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.0._block.3.weight\", \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.1._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._bottleneck._layers.1._block.3.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip2.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip2.bias\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip1.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_after_skip1.bias\", \"_object_decoder.spectrogram_reconstruction_network._conv_out.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_out.bias\". \n\tUnexpected key(s) in state_dict: \"_object_decoder.spectrogram_reconstruction_network.pre_vq_conv.weight\", \"_object_decoder.spectrogram_reconstruction_network.pre_vq_conv.bias\", \"_object_decoder.spectrogram_reconstruction_network.upblock1.weight\", \"_object_decoder.spectrogram_reconstruction_network.upblock1.bias\", \"_object_decoder.spectrogram_reconstruction_network.upblock2.weight\", \"_object_decoder.spectrogram_reconstruction_network.upblock2.bias\", \"_object_decoder.spectrogram_reconstruction_network._conv_1.weight\", \"_object_decoder.spectrogram_reconstruction_network._conv_1.bias\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.0._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.0._block.3.weight\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.1._block.1.weight\", \"_object_decoder.spectrogram_reconstruction_network._residual_stack._layers.1._block.3.weight\". \n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.0.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.0.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.3.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.encoder.block3.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up2.1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up2.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.0.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up3.1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.up3.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.db3.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_top._unet.decoder.fin_out.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.0.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.0.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.3.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.encoder.block3.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up2.1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up2.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.0.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.3.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db2.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up3.1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.up3.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.0.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 128, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.3.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.db3.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for _object_decoder._subspace_bot._unet.decoder.fin_out.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.0.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.3.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block1.3.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.0.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.3.weight: copying a param with shape torch.Size([1024, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network.block2.3.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans1.weight: copying a param with shape torch.Size([128, 64, 4, 4]) from checkpoint, the shape in current model is torch.Size([128, 128, 4, 4]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans2.weight: copying a param with shape torch.Size([64, 1, 4, 4]) from checkpoint, the shape in current model is torch.Size([128, 64, 4, 4]).\n\tsize mismatch for _object_decoder.spectrogram_reconstruction_network._conv_trans2.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "# Build and optionally load sDSR (same as stage2_analysis / scripts/train.py)\n",
    "from src.models.vq_vae.autoencoders import VQ_VAE_2Layer\n",
    "from src.models.sDSR.s_dsr import sDSR, sDSRConfig\n",
    "from src.utils.anomalies import AnomalyMapGenerator\n",
    "\n",
    "def build_vq_vae(n_mels: int, T: int):\n",
    "    return VQ_VAE_2Layer(\n",
    "        num_hiddens=128,\n",
    "        num_residual_layers=2,\n",
    "        num_residual_hiddens=64,\n",
    "        num_embeddings=(1024, 4096),\n",
    "        embedding_dim=128,\n",
    "        commitment_cost=0.25,\n",
    "        decay=0.99,\n",
    "    )\n",
    "\n",
    "def build_s_dsr(vq_vae, n_mels: int, T: int):\n",
    "    cfg = sDSRConfig(\n",
    "        embedding_dim=128,\n",
    "        num_hiddens=128,\n",
    "        num_residual_layers=2,\n",
    "        num_residual_hiddens=64,\n",
    "        n_mels=n_mels,\n",
    "        T=T,\n",
    "        use_subspace_restriction=True,\n",
    "    )\n",
    "    return sDSR(vq_vae, cfg)\n",
    "\n",
    "vq_vae = build_vq_vae(n_mels, T)\n",
    "load_trained = STAGE1_CKPT.exists()\n",
    "if load_trained:\n",
    "    ckpt_s1 = torch.load(STAGE1_CKPT, map_location=\"cpu\", weights_only=False)\n",
    "    vq_vae.load_state_dict(ckpt_s1[\"model_state_dict\"])\n",
    "model = build_s_dsr(vq_vae, n_mels, T)\n",
    "if STAGE2_CKPT.exists():\n",
    "    ckpt_s2 = torch.load(STAGE2_CKPT, map_location=\"cpu\", weights_only=False)\n",
    "    model.load_state_dict(ckpt_s2[\"model_state_dict\"])\n",
    "else:\n",
    "    print(\"Stage2 checkpoint not found; using random weights (signal-flow demo only).\")\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model ready (eval mode).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Anomaly map and mask visualization\n",
    "\n",
    "**2a.** Create a synthetic mask (e.g. rectangle in the center) at q_shape and visualize it at spectrogram size.  \n",
    "**2b.** Generate a few masks with `AnomalyMapGenerator` (Perlin and/or audio-specific) to show blob vs band patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Fake anomaly mask (controlled): rectangle at q_shape\n",
    "H_q, W_q = q_shape\n",
    "M_fake = torch.zeros(1, 1, H_q, W_q)\n",
    "# Rectangle in the center (e.g. 25% of each dimension)\n",
    "h_start, h_end = H_q // 4, 3 * H_q // 4\n",
    "w_start, w_end = W_q // 4, 3 * W_q // 4\n",
    "M_fake[:, :, h_start:h_end, w_start:w_end] = 1.0\n",
    "\n",
    "# Resize to spectrogram size for visualization\n",
    "M_fake_viz = F.interpolate(M_fake.float(), size=(n_mels, T), mode=\"nearest\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.imshow(M_fake_viz[0, 0].numpy(), aspect=\"auto\", cmap=\"gray\", origin=\"lower\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Mel\")\n",
    "ax.set_title(\"Fake anomaly mask (rectangle, at spectrogram size)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"M_fake shape: {M_fake.shape} (q_space); viz shape: {M_fake_viz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Generator-based masks (Perlin + audio-specific)\n",
    "gen = AnomalyMapGenerator(\n",
    "    strategy=\"both\",\n",
    "    spectrogram_shape=spectrogram_shape,\n",
    "    q_shape=q_shape,\n",
    "    n_mels=n_mels,\n",
    "    T=T,\n",
    "    zero_mask_prob=0.0,\n",
    ")\n",
    "M_perlin = gen.generate(2, \"cpu\", force_anomaly=True)\n",
    "M_audio = gen.generate(2, \"cpu\", force_anomaly=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "for i in range(2):\n",
    "    m = F.interpolate(M_perlin[i:i+1].float(), size=(n_mels, T), mode=\"nearest\")\n",
    "    axes[0, i].imshow(m[0, 0].numpy(), aspect=\"auto\", cmap=\"gray\", origin=\"lower\")\n",
    "    axes[0, i].set_title(f\"Generator mask (sample {i+1})\")\n",
    "for i in range(2):\n",
    "    m = F.interpolate(M_audio[i:i+1].float(), size=(n_mels, T), mode=\"nearest\")\n",
    "    axes[1, i].imshow(m[0, 0].numpy(), aspect=\"auto\", cmap=\"gray\", origin=\"lower\")\n",
    "    axes[1, i].set_title(f\"Generator mask (sample {i+3})\")\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Mel\")\n",
    "plt.suptitle(\"AnomalyMapGenerator (strategy='both', force_anomaly=True)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Two-branch decoder test with fake anomalies\n",
    "\n",
    "**3a.** Single-sample controlled run: one normal spectrogram `x`, known `M_gt`, then `forward_train(x, M_gt=M_fake)`.  \n",
    "**3b.** Visual comparison: input **x**, **x_g** (with anomaly), **x_s** (without anomaly), and **M**.  \n",
    "**3c.** Quantitative: mean |x_g − x| and |x_s − x| inside vs outside mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. Single-sample controlled run\n",
    "# Synthetic normal spectrogram (smooth pattern so \"normal\" is clear)\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, 1, n_mels, T, device=DEVICE) * 0.5\n",
    "# Optional: make it smoother for clearer visuals\n",
    "x = F.avg_pool2d(x, 5, stride=1, padding=2)\n",
    "\n",
    "# Known mask at q_shape (same rectangle as in section 2)\n",
    "M_gt = M_fake.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.forward_train(x, M_gt=M_gt)\n",
    "\n",
    "x_in = out[\"x\"]\n",
    "x_g = out[\"x_g\"]\n",
    "x_s = out[\"x_s\"]\n",
    "M_for_viz = out[\"M\"]\n",
    "\n",
    "print(\"Shapes: x\", x_in.shape, \"x_g\", x_g.shape, \"x_s\", x_s.shape, \"M\", M_for_viz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. Visual comparison: x (normal), x_g (with anomaly), x_s (without anomaly), M\n",
    "vmin = min(x_in.min().item(), x_g.min().item(), x_s.min().item())\n",
    "vmax = max(x_in.max().item(), x_g.max().item(), x_s.max().item())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axes[0, 0].imshow(x_in[0, 0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "axes[0, 0].set_title(\"Input x (normal)\")\n",
    "axes[0, 1].imshow(x_g[0, 0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "axes[0, 1].set_title(\"x_g (general decoder, with anomalies)\")\n",
    "axes[1, 0].imshow(x_s[0, 0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "axes[1, 0].set_title(\"x_s (object-specific decoder, without anomalies)\")\n",
    "axes[1, 1].imshow(M_for_viz[0, 0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", cmap=\"gray\")\n",
    "axes[1, 1].set_title(\"Mask M (for reference)\")\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Mel\")\n",
    "plt.suptitle(\"Two-branch decoder: x_g vs x_s (same color scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. Quantitative check: mean absolute difference inside vs outside mask\n",
    "M_bool = (M_for_viz[0, 0] > 0.5).float()\n",
    "diff_g = (x_g[0, 0] - x_in[0, 0]).abs()\n",
    "diff_s = (x_s[0, 0] - x_in[0, 0]).abs()\n",
    "\n",
    "inside = M_bool > 0.5\n",
    "outside = 1.0 - M_bool\n",
    "n_in = inside.sum().clamp(min=1).item()\n",
    "n_out = outside.sum().clamp(min=1).item()\n",
    "\n",
    "mad_g_in = (diff_g * inside).sum().item() / n_in\n",
    "mad_g_out = (diff_g * outside).sum().item() / n_out\n",
    "mad_s_in = (diff_s * inside).sum().item() / n_in\n",
    "mad_s_out = (diff_s * outside).sum().item() / n_out\n",
    "\n",
    "print(\"Mean absolute difference (general decoder x_g vs input x):\")\n",
    "print(f\"  Inside mask:  {mad_g_in:.4f}\")\n",
    "print(f\"  Outside mask: {mad_g_out:.4f}\")\n",
    "print(\"Mean absolute difference (object decoder x_s vs input x):\")\n",
    "print(f\"  Inside mask:  {mad_s_in:.4f}\")\n",
    "print(f\"  Outside mask: {mad_s_out:.4f}\")\n",
    "print(\"Expected: inside mask, diff_g large (anomaly in x_g), diff_s smaller (x_s reconstructs normal).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch with mixed normal / anomalous (mirror training)\n",
    "\n",
    "Create a batch of 4: first 2 with M=0 (normal), last 2 with a fixed fake M. Pass explicit `M_gt` and run `forward_train(x_batch, M_gt=M_gt)`. Plot one sample from the normal half and one from the anomalous half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of 4: first 2 normal (M=0), last 2 with fake anomaly mask\n",
    "batch_size = 4\n",
    "half = batch_size // 2\n",
    "torch.manual_seed(123)\n",
    "x_batch = torch.randn(batch_size, 1, n_mels, T, device=DEVICE) * 0.5\n",
    "x_batch = F.avg_pool2d(x_batch, 5, stride=1, padding=2)\n",
    "\n",
    "M_zeros = torch.zeros(half, 1, H_q, W_q, device=DEVICE)\n",
    "M_fake_batch = M_fake.expand(half, 1, H_q, W_q).to(DEVICE)\n",
    "M_gt_batch = torch.cat([M_zeros, M_fake_batch], dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_batch = model.forward_train(x_batch, M_gt=M_gt_batch)\n",
    "\n",
    "x_b = out_batch[\"x\"]\n",
    "x_g_b = out_batch[\"x_g\"]\n",
    "x_s_b = out_batch[\"x_s\"]\n",
    "M_b = out_batch[\"M\"]\n",
    "print(\"Batch shapes:\", x_b.shape, x_g_b.shape, x_s_b.shape, M_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: one sample from normal half (idx 0), one from anomalous half (idx 2)\n",
    "vmin = x_b.min().item()\n",
    "vmax = x_b.max().item()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "# Normal half: sample 0\n",
    "for j, (tensor, title) in enumerate([\n",
    "    (x_b[0], \"x (input)\"),\n",
    "    (x_g_b[0], \"x_g\"),\n",
    "    (x_s_b[0], \"x_s\"),\n",
    "    (M_b[0], \"M\"),\n",
    "]):\n",
    "    ax = axes[0, j]\n",
    "    if \"M\" in title:\n",
    "        ax.imshow(tensor[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", cmap=\"gray\")\n",
    "    else:\n",
    "        ax.imshow(tensor[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"Normal half [0] — {title}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Mel\")\n",
    "# Anomalous half: sample 2\n",
    "for j, (tensor, title) in enumerate([\n",
    "    (x_b[2], \"x (input)\"),\n",
    "    (x_g_b[2], \"x_g\"),\n",
    "    (x_s_b[2], \"x_s\"),\n",
    "    (M_b[2], \"M\"),\n",
    "]):\n",
    "    ax = axes[1, j]\n",
    "    if \"M\" in title:\n",
    "        ax.imshow(tensor[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", cmap=\"gray\")\n",
    "    else:\n",
    "        ax.imshow(tensor[0].cpu().numpy(), aspect=\"auto\", origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"Anomalous half [2] — {title}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Mel\")\n",
    "plt.suptitle(\"Batch: normal half vs anomalous half (x_g with anomaly, x_s closer to normal)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training-phase pipeline (step-by-step)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  subgraph encode [Encode]\n",
    "    X[\"x (normal)\"] --> Enc[\"encoder\"]\n",
    "    Enc --> Q[\"q_bot, q_top\"]\n",
    "  end\n",
    "  Q --> Anom[\"anomaly module + M\"]\n",
    "  Anom --> Qa[\"q_bot_a, q_top_a\"]\n",
    "  Qa --> GenDec[\"general decoder\"]\n",
    "  Qa --> ObjDec[\"object decoder\"]\n",
    "  GenDec --> Xg[\"x_g (with anomalies)\"]\n",
    "  ObjDec --> Xs[\"x_s (without anomalies)\"]\n",
    "  Xg --> Focal[\"focal loss vs M\"]\n",
    "  Xs --> L2[\"L2 loss vs x\"]\n",
    "```\n",
    "- **Branch 1:** General decoder gets anomalous q → **x_g** (spectrogram with anomalies).  \n",
    "- **Branch 2:** Object decoder gets same anomalous q, trained with L2 to match normal **x** → **x_s** (spectrogram without anomalies).  \n",
    "- **M** is the anomaly mask; used as target for segmentation (focal loss)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
