{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference comparison: Normal vs synthetic vs real anomalous\n",
        "\n",
        "Load a **normal** spectrogram, a **synthetically generated anomalous** sample (normal + synthetic mask through `forward_train`), and a **real anomalous** sample from the test set; feed them through the trained stage-2 (sDSR) model and plot all inference steps for comparison.\n",
        "\n",
        "Requires: stage1 checkpoint, stage2 checkpoint (e.g. fan), and DCASE2020 Task 2 dev data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and paths\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "_cwd = Path(\".\").resolve()\n",
        "PROJECT_ROOT = _cwd.parent if _cwd.name == \"notebooks\" else _cwd\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "MACHINE_TYPE = \"fan\"\n",
        "DATA_PATH = PROJECT_ROOT / \"../data/dcase2020-task2-dev-dataset\"  # adjust to your DCASE root\n",
        "CKPT_DIR = PROJECT_ROOT / \"checkpoints\"\n",
        "# Stage1: use multi-machine or single-machine ckpt (e.g. fan-only)\n",
        "STAGE1_CKPT = CKPT_DIR / \"stage1\" / \"ToyCar+ToyConveyor+fan+pump+slider+valve\" / \"stage1_ToyCar+ToyConveyor+fan+pump+slider+valve_final.pt\"\n",
        "if not STAGE1_CKPT.exists():\n",
        "    STAGE1_CKPT = CKPT_DIR / \"stage1\" / MACHINE_TYPE / \"stage1_fan_best.pt\"\n",
        "STAGE2_CKPT = CKPT_DIR / \"stage2\" / MACHINE_TYPE / \"stage2_fan_best.pt\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Stage1 exists: {STAGE1_CKPT.exists()}\")\n",
        "print(f\"Stage2 exists: {STAGE2_CKPT.exists()}\")\n",
        "print(f\"Data path exists: {Path(DATA_PATH).exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Datasets and model loading\n",
        "\n",
        "Train dataset (fan only) defines normalization: mean and std are computed per machine_type. Test dataset is given these so that spectrograms returned by `test_ds[i]` are already normalized with fan's statistics — consistent with the evaluator and with training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data.dataset import DCASE2020Task2LogMelDataset, DCASE2020Task2TestDataset\n",
        "from src.models.vq_vae.autoencoders import VQ_VAE_2Layer\n",
        "from src.models.sDSR.s_dsr import sDSR, sDSRConfig\n",
        "\n",
        "train_ds = DCASE2020Task2LogMelDataset(\n",
        "    root=str(DATA_PATH),\n",
        "    machine_type=MACHINE_TYPE,\n",
        "    normalize=True,\n",
        ")\n",
        "_, _, n_mels, T = train_ds.data.shape\n",
        "\n",
        "# test_ds uses fan train mean/std so __getitem__ returns normalized spectrograms\n",
        "test_ds = DCASE2020Task2TestDataset(\n",
        "    root=str(DATA_PATH),\n",
        "    machine_type=MACHINE_TYPE,\n",
        "    mean=train_ds.mean,\n",
        "    std=train_ds.std,\n",
        "    target_T=train_ds.target_T,\n",
        ")\n",
        "\n",
        "vq_vae = VQ_VAE_2Layer(\n",
        "    num_hiddens=128,\n",
        "    num_residual_layers=2,\n",
        "    num_residual_hiddens=64,\n",
        "    num_embeddings=(1024, 4096),\n",
        "    embedding_dim=128,\n",
        "    commitment_cost=0.25,\n",
        "    decay=0.99,\n",
        ")\n",
        "stage1 = torch.load(STAGE1_CKPT, map_location=\"cpu\", weights_only=True)\n",
        "vq_vae.load_state_dict(stage1[\"model_state_dict\"])\n",
        "\n",
        "cfg = sDSRConfig(embedding_dim=128, num_hiddens=128, n_mels=n_mels, T=T)\n",
        "model = sDSR(vq_vae, cfg)\n",
        "stage2 = torch.load(STAGE2_CKPT, map_location=\"cpu\", weights_only=True)\n",
        "model.load_state_dict(stage2[\"model_state_dict\"])\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "print(f\"n_mels={n_mels}, T={T}\")\n",
        "print(f\"Test set: {len(test_ds)} clips\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sample selection\n",
        "\n",
        "Pick one **normal** and one **real anomalous** clip from the test set. Use another normal clip for the **synthetic anomalous** run (we will apply a synthetic mask via `forward_train`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normal_idx = None\n",
        "anomalous_idx = None\n",
        "for i in range(len(test_ds)):\n",
        "    _, label, _ = test_ds.samples[i]\n",
        "    if label == 0 and normal_idx is None:\n",
        "        normal_idx = i\n",
        "    if label == 1 and anomalous_idx is None:\n",
        "        anomalous_idx = i\n",
        "    if normal_idx is not None and anomalous_idx is not None:\n",
        "        break\n",
        "\n",
        "spec_normal, label_n, mid_n = test_ds[normal_idx]\n",
        "spec_anomalous, label_a, mid_a = test_ds[anomalous_idx]\n",
        "\n",
        "# Batch of 1: (1, 1, n_mels, T); test returns (1, n_mels, T) so one unsqueeze(0)\n",
        "x_normal = spec_normal.unsqueeze(0).to(DEVICE)\n",
        "x_anomalous = spec_anomalous.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "# For synthetic: same normal input + synthetic mask (generated below)\n",
        "x_synthetic_input = x_normal.clone()\n",
        "\n",
        "print(f\"Normal: idx={normal_idx}, label={label_n}, machine_id={mid_n}, shape={x_normal.shape}\")\n",
        "print(f\"Real anomalous: idx={anomalous_idx}, label={label_a}, machine_id={mid_a}, shape={x_anomalous.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Synthetic anomaly mask\n",
        "\n",
        "Generate one synthetic anomaly mask (same strategy as stage-2 training) and format it as `(1, 1, n_mels, T)` for `forward_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.utils.anomalies import AnomalyMapGenerator\n",
        "\n",
        "mask_gen = AnomalyMapGenerator(\n",
        "    strategy=\"both\",\n",
        "    spectrogram_shape=(n_mels, T),\n",
        "    q_shape=(n_mels, T),\n",
        "    n_mels=n_mels,\n",
        "    T=T,\n",
        "    zero_mask_prob=0.0,\n",
        ")\n",
        "M_synth = mask_gen.generate(1, device=\"cpu\", force_anomaly=True)  # (1, 1, n_mels, T)\n",
        "M_synth = M_synth.to(DEVICE)\n",
        "print(f\"Synthetic mask shape: {M_synth.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run inference\n",
        "\n",
        "- **Normal** and **real anomalous**: `forward(x, return_intermediates=True)` → M_out, X_G, X_S.\n",
        "- **Synthetic anomalous**: `forward_train(x_normal, M_synth)` → same outputs with codebook-replaced codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Normal: standard inference path\n",
        "    m_out_n, x_g_n, x_s_n = model(x_normal, return_intermediates=True)\n",
        "\n",
        "    # Real anomalous: same inference path, different input\n",
        "    m_out_a, x_g_a, x_s_a = model(x_anomalous, return_intermediates=True)\n",
        "\n",
        "    # Synthetic anomalous: training path with synthetic mask (model in eval mode)\n",
        "    out_synth = model.forward_train(x_synthetic_input, M_synth)\n",
        "    m_out_s = out_synth[\"m_out\"]\n",
        "    x_g_s = out_synth[\"x_g\"]\n",
        "    x_s_s = out_synth[\"x_s\"]\n",
        "\n",
        "# Detach and move to CPU for plotting\n",
        "def to_np(t):\n",
        "    return t.detach().cpu().squeeze().numpy()\n",
        "\n",
        "inputs = {\n",
        "    \"Normal\": to_np(x_normal),\n",
        "    \"Synthetic anomalous\": to_np(x_synthetic_input),\n",
        "    \"Real anomalous\": to_np(x_anomalous),\n",
        "}\n",
        "x_g = {\"Normal\": to_np(x_g_n), \"Synthetic anomalous\": to_np(x_g_s), \"Real anomalous\": to_np(x_g_a)}\n",
        "x_s = {\"Normal\": to_np(x_s_n), \"Synthetic anomalous\": to_np(x_s_s), \"Real anomalous\": to_np(x_s_a)}\n",
        "m_out = {\"Normal\": to_np(m_out_n), \"Synthetic anomalous\": to_np(m_out_s), \"Real anomalous\": to_np(m_out_a)}\n",
        "\n",
        "# Anomaly logit is channel 1\n",
        "anomaly_logit = {k: v[1] if v.ndim == 3 else v for k, v in m_out.items()}\n",
        "diff_xg_xs = {k: np.abs(x_g[k] - x_s[k]) for k in x_g}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Figures: compare all steps\n",
        "\n",
        "For each sample type (normal, synthetic anomalous, real anomalous), plot: **Input**, **X_G** (general decoder), **X_S** (object-specific decoder), **|X_G − X_S|**, and **anomaly score map** (M_out channel 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titles = [\"Normal\", \"Synthetic anomalous\", \"Real anomalous\"]\n",
        "keys = list(inputs.keys())\n",
        "\n",
        "fig, axes = plt.subplots(5, 3, figsize=(12, 14))\n",
        "fig.suptitle(\"Inference steps: Normal vs Synthetic vs Real anomalous (fan)\", fontsize=12)\n",
        "\n",
        "for col, key in enumerate(keys):\n",
        "    axes[0, col].imshow(inputs[key], aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n",
        "    axes[0, col].set_title(titles[col])\n",
        "    axes[0, col].set_ylabel(\"Input X\")\n",
        "\n",
        "    axes[1, col].imshow(x_g[key], aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n",
        "    axes[1, col].set_ylabel(\"X_G\")\n",
        "\n",
        "    axes[2, col].imshow(x_s[key], aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n",
        "    axes[2, col].set_ylabel(\"X_S\")\n",
        "\n",
        "    axes[3, col].imshow(diff_xg_xs[key], aspect=\"auto\", origin=\"lower\", cmap=\"hot\")\n",
        "    axes[3, col].set_ylabel(\"|X_G − X_S|\")\n",
        "\n",
        "    axes[4, col].imshow(anomaly_logit[key], aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\n",
        "    axes[4, col].set_ylabel(\"Anomaly logit\")\n",
        "\n",
        "for ax in axes.flat:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Optional: plot synthetic mask\n",
        "\n",
        "Visualize the synthetic anomaly mask used for the middle column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
        "ax.imshow(to_np(M_synth).squeeze(), aspect=\"auto\", origin=\"lower\", cmap=\"gray\")\n",
        "ax.set_title(\"Synthetic anomaly mask M (used for forward_train)\")\n",
        "ax.set_xlabel(\"Time\")\n",
        "ax.set_ylabel(\"Mel\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
